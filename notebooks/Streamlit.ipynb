{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G6B6Xzh-ePfO",
        "outputId": "422fe638-5330-4798-f8fb-3762a6c054f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning==1.5.10\n",
            "  Downloading pytorch_lightning-1.5.10-py3-none-any.whl (527 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.7/527.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10) (2.1.0+cu118)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10) (0.18.3)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10) (2023.6.0)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10) (2.14.1)\n",
            "Collecting torchmetrics>=0.4.1 (from pytorch-lightning==1.5.10)\n",
            "  Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyDeprecate==0.3.1 (from pytorch-lightning==1.5.10)\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10) (23.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.5.10) (4.5.0)\n",
            "Collecting setuptools==59.5.0 (from pytorch-lightning==1.5.10)\n",
            "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m952.4/952.4 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (3.9.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (1.59.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (3.5.1)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (3.20.3)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.*->pytorch-lightning==1.5.10) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.*->pytorch-lightning==1.5.10) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.*->pytorch-lightning==1.5.10) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.*->pytorch-lightning==1.5.10) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.*->pytorch-lightning==1.5.10) (2.1.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics>=0.4.1->pytorch-lightning==1.5.10)\n",
            "  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (4.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.*->pytorch-lightning==1.5.10) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (3.2.2)\n",
            "Installing collected packages: setuptools, pyDeprecate, lightning-utilities, torchmetrics, pytorch-lightning\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "arviz 0.15.1 requires setuptools>=60.0.0, but you have setuptools 59.5.0 which is incompatible.\n",
            "cvxpy 1.3.2 requires setuptools>65.5.1, but you have setuptools 59.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lightning-utilities-0.10.0 pyDeprecate-0.3.1 pytorch-lightning-1.5.10 setuptools-59.5.0 torchmetrics-1.2.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 2.594s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit\n",
        "!pip install pytorch-lightning==1.5.10\n",
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.datasets import FashionMNIST\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import zipfile\n",
        "\n",
        "# random_seed = 42\n",
        "# torch.manual_seed(random_seed)\n",
        "BATCH_SIZE = 128\n",
        "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
        "NUM_WORKERS = int(os.cpu_count() / 2)\n",
        "\n",
        "class FMNISTDataModule(pl.LightningDataModule):\n",
        "  def __init__ (self, data_dir='./data', batch_size = BATCH_SIZE, num_workers=NUM_WORKERS):\n",
        "    \"\"\"\n",
        "    Initializes the FMNISTDataModule.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): Directory to store the dataset.\n",
        "        batch_size (int): Size of the mini-batches during training.\n",
        "        num_workers (int): Number of workers for DataLoader.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.data_dir = data_dir\n",
        "    self.batch_size = batch_size\n",
        "    self.num_workers = num_workers\n",
        "\n",
        "    # Define the data transformation pipeline\n",
        "    self.transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "  def prepare_data(self):\n",
        "    \"\"\"\n",
        "    Downloads and prepares the FashionMNIST dataset.\n",
        "    \"\"\"\n",
        "    FashionMNIST(self.data_dir, train=True, download=True)\n",
        "    FashionMNIST(self.data_dir, train=False, download=True)\n",
        "\n",
        "  def setup(self, stage=None):\n",
        "    \"\"\"\n",
        "    Splits the dataset into training, validation, and test sets.\n",
        "\n",
        "    Args:\n",
        "        stage (str, optional): 'fit' for training/validation, 'test' for testing.\n",
        "    \"\"\"\n",
        "    if stage==\"fit\" or stage is None:\n",
        "      fmnist_full = FashionMNIST(self.data_dir, train=True, transform=self.transform)\n",
        "      self.fmnist_train, self.fmnist_val = random_split(fmnist_full, [55000, 5000])\n",
        "\n",
        "    if stage==\"test\" or stage is None:\n",
        "      self.fmnist_test = FashionMNIST(self.data_dir, train=False, transform=self.transform)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    \"\"\"\n",
        "    Returns DataLoader for the training set.\n",
        "    \"\"\"\n",
        "    return DataLoader(self.fmnist_train, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    \"\"\"\n",
        "    Returns DataLoader for the validation set.\n",
        "    \"\"\"\n",
        "    return DataLoader(self.fmnist_val, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    \"\"\"\n",
        "    Returns DataLoader for the test set.\n",
        "    \"\"\"\n",
        "    return DataLoader(self.fmnist_test, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "\n",
        "class MNISTDataModule(pl.LightningDataModule):\n",
        "  def __init__ (self, data_dir='./data', batch_size = BATCH_SIZE, num_workers=NUM_WORKERS):\n",
        "    \"\"\"\n",
        "    Initializes the MNISTDataModule.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): Directory to store the dataset.\n",
        "        batch_size (int): Size of the mini-batches during training.\n",
        "        num_workers (int): Number of workers for DataLoader.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.data_dir = data_dir\n",
        "    self.batch_size = batch_size\n",
        "    self.num_workers = num_workers\n",
        "\n",
        "    self.transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "  def prepare_data(self):\n",
        "    \"\"\"\n",
        "    Downloads and prepares the MNIST dataset.\n",
        "    \"\"\"\n",
        "    MNIST(self.data_dir, train=True, download=False)\n",
        "    MNIST(self.data_dir, train=False, download=True)\n",
        "\n",
        "  def setup(self, stage=None):\n",
        "    \"\"\"\n",
        "    Splits the dataset into training, validation, and test sets.\n",
        "\n",
        "    Args:\n",
        "        stage (str, optional): 'fit' for training/validation, 'test' for testing.\n",
        "    \"\"\"\n",
        "    if stage==\"fit\" or stage is None:\n",
        "      mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
        "      self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
        "\n",
        "    if stage==\"test\" or stage is None:\n",
        "      self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    \"\"\"\n",
        "    Returns DataLoader for the training set.\n",
        "    \"\"\"\n",
        "    return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    \"\"\"\n",
        "    Returns DataLoader for the validation set.\n",
        "    \"\"\"\n",
        "    return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    \"\"\"\n",
        "    Returns DataLoader for the test set.\n",
        "    \"\"\"\n",
        "    return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "# Define the Discriminator neural network class\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Initializes the Discriminator neural network.\n",
        "\n",
        "    The architecture consists of convolutional layers, max-pooling, dropout, and fully connected layers.\n",
        "\n",
        "    Input: 1 channel (grayscale image)\n",
        "    Output: 1 (sigmoid activation for binary classification)\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # First convolutional layer with 5x5 kernel, input channels=1, output channels=10\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "\n",
        "    # Second convolutional layer with 5x5 kernel, input channels=10, output channels=20\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "\n",
        "    # Dropout layer to prevent overfitting\n",
        "    self.conv2d_drop = nn.Dropout2d()\n",
        "\n",
        "    # Fully connected layers\n",
        "    self.fc1 = nn.Linear(320, 50)     # 320 input features, 50 output features\n",
        "    self.fc2 = nn.Linear(50, 1)       # 50 input features, 1 output feature\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Defines the forward pass of the Discriminator.\n",
        "\n",
        "    Args:\n",
        "        x (torch.Tensor): Input tensor.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Output tensor with sigmoid activation for binary classification.\n",
        "    \"\"\"\n",
        "\n",
        "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "    x = F.relu(F.max_pool2d(self.conv2d_drop(self.conv2(x)), 2))\n",
        "\n",
        "    # Reshape the tensor for the fully connected layers\n",
        "    x = x.view(-1, 320)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.dropout(x, training=self.training)\n",
        "\n",
        "    # Final fully connected layer with sigmoid activation for binary classification\n",
        "    x = self.fc2(x)\n",
        "    return torch.sigmoid(x)\n",
        "\n",
        "# Define the Generator neural network class\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, latent_dim):\n",
        "    \"\"\"\n",
        "    Initializes the Generator neural network.\n",
        "\n",
        "    The architecture consists of linear and transpose convolutional layers to generate images.\n",
        "\n",
        "    Args:\n",
        "        latent_dim (int): Dimensionality of the latent space.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # Linear layer to transform the latent space to the initial convolutional shape\n",
        "    self.lin1 = nn.Linear(latent_dim, 7*7*64)\n",
        "\n",
        "    # Transpose convolutional layers to upsample the data\n",
        "    self.ct1 = nn.ConvTranspose2d(64, 32, 4, stride=2)\n",
        "    self.ct2 = nn.ConvTranspose2d(32, 16, 4, stride=2)\n",
        "\n",
        "    # Final convolutional layer to generate the output image\n",
        "    self.conv = nn.Conv2d(16, 1, kernel_size=7)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Defines the forward pass of the Generator.\n",
        "\n",
        "    Args:\n",
        "        x (torch.Tensor): Input tensor from the latent space.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Output tensor representing the generated image.\n",
        "    \"\"\"\n",
        "    # Linear layer to transform the input from latent space to initial convolutional shape\n",
        "    x = self.lin1(x)\n",
        "    x = F.relu(x)\n",
        "\n",
        "    # Reshape the tensor for the transpose convolutional layers\n",
        "    x = x.view(-1, 64, 7, 7)\n",
        "\n",
        "    x = self.ct1(x)\n",
        "    x = F.relu(x)\n",
        "\n",
        "    x = self.ct2(x)\n",
        "    x = F.relu(x)\n",
        "\n",
        "    # Final convolutional layer to generate the output image\n",
        "    return self.conv(x)\n",
        "\n",
        "\n",
        "# Define the GAN LightningModule class\n",
        "class GAN(pl.LightningModule):\n",
        "\n",
        "  def __init__(self, latent_dim=100, lr=0.0002):\n",
        "    \"\"\"\n",
        "    Initializes the GAN LightningModule.\n",
        "\n",
        "    Args:\n",
        "        latent_dim (int): Dimensionality of the latent space.\n",
        "        lr (float): Learning rate for optimization.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.save_hyperparameters()\n",
        "\n",
        "    self.generator = Generator(latent_dim=self.hparams.latent_dim)\n",
        "    self.discriminator = Discriminator()\n",
        "\n",
        "    # Pre-generate random noise for validation\n",
        "    self.validation_z = torch.randn(6, self.hparams.latent_dim)\n",
        "\n",
        "  def forward(self, z):\n",
        "    \"\"\"\n",
        "    Defines the forward pass of the GAN.\n",
        "\n",
        "    Args:\n",
        "        z (torch.Tensor): Input tensor from the latent space.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Output tensor representing the generated image.\n",
        "    \"\"\"\n",
        "    return self.generator(z)\n",
        "\n",
        "  def adversarial_loss(self, y_hat, y):\n",
        "    \"\"\"\n",
        "    Computes the adversarial loss using binary cross-entropy.\n",
        "\n",
        "    Args:\n",
        "        y_hat (torch.Tensor): Predictions.\n",
        "        y (torch.Tensor): Ground truth labels.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Adversarial loss.\n",
        "    \"\"\"\n",
        "    return F.binary_cross_entropy(y_hat, y)\n",
        "\n",
        "  def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "    \"\"\"\n",
        "    Defines the training step for both the Generator and the Discriminator.\n",
        "\n",
        "    Args:\n",
        "        batch: Batch of real images.\n",
        "        batch_idx: Index of the current batch.\n",
        "        optimizer_idx: Index of the optimizer being used (0 for Generator, 1 for Discriminator).\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing the loss and logging information.\n",
        "    \"\"\"\n",
        "\n",
        "    # Getting real images from the batch\n",
        "    real_imgs, _ = batch\n",
        "\n",
        "    # Creating random noise\n",
        "    z = torch.randn(real_imgs.shape[0], self.hparams.latent_dim)\n",
        "    z = z.type_as(real_imgs)\n",
        "\n",
        "    # For Generator\n",
        "    if optimizer_idx == 0:\n",
        "\n",
        "      # The forward step - which creates images using generator from random noise\n",
        "      fake_imgs = self(z)\n",
        "\n",
        "      # The output from the discriminator\n",
        "      # 1 means the discriminator predicted this as a real image, meaning the generator did a great job\n",
        "      # 0 means the discriminator predicted this as a fake image. The generator needs to improve\n",
        "      y_hat = self.discriminator(fake_imgs)\n",
        "      y = torch.ones(real_imgs.size(0), 1)\n",
        "\n",
        "      y = y.type_as(real_imgs)\n",
        "\n",
        "      g_loss = self.adversarial_loss(y_hat, y)\n",
        "\n",
        "      log_dict = {\"g_loss\": g_loss}\n",
        "\n",
        "      return {\"loss\": g_loss, \"progress_bar\": log_dict, \"log\": log_dict}\n",
        "\n",
        "    # For Discriminator\n",
        "    if optimizer_idx == 1:\n",
        "      y_hat_real = self.discriminator(real_imgs)\n",
        "\n",
        "      y_real = torch.ones(real_imgs.size(0), 1)\n",
        "      y_real = y_real.type_as(real_imgs)\n",
        "\n",
        "      real_loss = self.adversarial_loss(y_hat_real, y_real)\n",
        "\n",
        "      y_hat_fake = self.discriminator(self(z).detach())\n",
        "      y_fake = torch.zeros(real_imgs.size(0), 1)\n",
        "      y_fake = y_fake.type_as(real_imgs)\n",
        "\n",
        "      fake_loss = self.adversarial_loss(y_hat_fake, y_fake)\n",
        "\n",
        "      d_loss = (real_loss + fake_loss) / 2\n",
        "      log_dict = {\"d_loss\": d_loss}\n",
        "\n",
        "      return {\"loss\": d_loss, \"progress_bar\": log_dict, \"log\": log_dict}\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    \"\"\"\n",
        "    Configures the optimizers for the Generator and Discriminator.\n",
        "\n",
        "    Returns:\n",
        "        list: List of optimizers.\n",
        "    \"\"\"\n",
        "    lr = self.hparams.lr\n",
        "    opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr)\n",
        "    opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr)\n",
        "    return [opt_g, opt_d], []\n",
        "\n",
        "  def plot_imgs(self):\n",
        "    \"\"\"\n",
        "    Plots generated images using the current Generator state.\n",
        "    \"\"\"\n",
        "    z = self.validation_z.type_as(self.generator.lin1.weight)\n",
        "    sample_imgs = self(z).cpu()\n",
        "\n",
        "    print('epoch ', self.current_epoch)\n",
        "    fig = plt.figure()\n",
        "\n",
        "    for i in range(sample_imgs.size(0)):\n",
        "      plt.subplot(2, 3, i + 1)\n",
        "      plt.tight_layout()\n",
        "      plt.imshow(sample_imgs.detach()[i, 0, :, :], cmap = 'gray_r', interpolation = 'none')\n",
        "      plt.title(\"Generated Data\")\n",
        "      plt.xticks([])\n",
        "      plt.yticks([])\n",
        "      plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "  def on_epoch_end(self):\n",
        "    \"\"\"\n",
        "    Hook function called at the end of each epoch.\n",
        "    \"\"\"\n",
        "    self.plot_imgs()\n",
        "\n",
        "\n",
        "def download_img(ds_type, number):\n",
        "  \"\"\"\n",
        "  Downloads and saves generated images from a trained GAN model.\n",
        "\n",
        "  Args:\n",
        "      ds_type (str): Dataset type ('Numbers' or 'Fashion').\n",
        "      number (int): Number of images to generate and save.\n",
        "  \"\"\"\n",
        "  TRANSFORM = transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "  # This is just to tell the number of fake images to generate\n",
        "  mnist_val = MNIST(\"./data\", download=True, train=False, transform=TRANSFORM)\n",
        "  for batch in DataLoader(mnist_val, batch_size=number, num_workers=NUM_WORKERS):\n",
        "    img, _ = batch\n",
        "    break\n",
        "\n",
        "  # Generate random noise for the latent space\n",
        "  z = torch.randn(img.shape[0], 100)\n",
        "  z = z.type_as(img)\n",
        "\n",
        "  # Load the GAN model based on the dataset type\n",
        "  if ds_type == 'Numbers':\n",
        "    model2 = GAN.load_from_checkpoint(checkpoint_path=\"/content/drive/MyDrive/Saved_Models/GAN/GAN1.ckpt\")\n",
        "\n",
        "  elif ds_type == 'Fashion':\n",
        "    model2 = GAN.load_from_checkpoint(checkpoint_path=\"/content/drive/MyDrive/Saved_Models/GAN/GAN2.ckpt\")\n",
        "\n",
        "  else:\n",
        "    return\n",
        "\n",
        "  # Generate fake images using the loaded GAN model\n",
        "  fake_imgs = model2(z)\n",
        "\n",
        "  if os.path.exists('/content/For_Download') == False:\n",
        "    os.mkdir('/content/For_Download')\n",
        "\n",
        "  # Save each generated image and display a random one\n",
        "  rand_int = np.random.randint(0, number)\n",
        "  for i in range(fake_imgs.size(0)):\n",
        "    fig = plt.figure()\n",
        "    plt.imshow(fake_imgs.detach()[i, 0, :, :], interpolation = 'none')\n",
        "    plt.savefig(f\"/content/For_Download/Img{i}.png\")\n",
        "    if i == rand_int:\n",
        "      st.image(f'/content/For_Download/Img{i}.png', caption='Here is a sample image')\n",
        "\n",
        "  # Create a ZipFile object for the folder that you want to zip\n",
        "  zip_file = zipfile.ZipFile('/content/For_Download.zip', 'w')\n",
        "\n",
        "  # Add all of the files in the folder to the zip file\n",
        "  for file in os.listdir('/content/For_Download'):\n",
        "      zip_file.write(os.path.join('/content/For_Download', file))\n",
        "\n",
        "  # Close the zip file\n",
        "  zip_file.close()\n",
        "\n",
        "# Display a header with a rainbow divider\n",
        "st.header('Training Data Augmentation using GANs', divider='rainbow')\n",
        "\n",
        "# Create a form with a selectbox for dataset type and a slider for the number of images\n",
        "with st.form('my_form'):\n",
        "  ds_type = st.selectbox('For which dataset do you need more images?', ('None', 'Numbers', 'Fashion'))\n",
        "  number = st.slider(\"Number of images you want\", min_value=1, max_value=100, value=None)\n",
        "  submitted = st.form_submit_button('Submit')\n",
        "  st.write('Dataset you want more images for augmentation:', ds_type)\n",
        "  st.write('Number of images you want:', number)\n",
        "  download_img(ds_type, number)\n",
        "\n",
        "if ds_type != \"None\":\n",
        "  st.write(\"Your images are ready for download!\")\n",
        "  with open('/content/For_Download.zip', 'rb') as f:\n",
        "    # Provide a download button for the generated zip file\n",
        "    st.download_button('Download Zip', f, file_name='Images.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6kQlCjOeXGP",
        "outputId": "d669275f-ef9a-4325-c149-ac966fb41e59"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QCMZebua2L_G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxEh7ZuQeiP8",
        "outputId": "c1e78b9c-4497-476b-f0ad-c21daec3570b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.243.146.98\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.109s\n",
            "your url is: https://rich-crabs-sin.loca.lt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iw4IZOQbetYT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}